{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c62221c-ebed-4e0d-ade4-5a0bd67f551b",
   "metadata": {},
   "source": [
    "# [3-tuning] - Parametrages de hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753d4ad-72aa-4cca-880b-aaa7553d455e",
   "metadata": {},
   "source": [
    "## Import des modules\n",
    "> cf. [pyproject.toml](pyproject.toml) pour connaître les librairies à installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85e4ff-acdf-448b-aad3-be76873aa8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fccce9-66c5-455c-87bb-568ca1916a55",
   "metadata": {},
   "source": [
    "## Récupération des données dans le stockage objet AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117fa22a-2312-498a-9e3c-67c169ff814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configutations S3 access and data\n",
    "load_dotenv()\n",
    "aws_access_key_id = os.getenv('aws_access_key_id')\n",
    "aws_secret_access_key = os.getenv('aws_secret_access_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fce538-10ad-4d00-8ffb-a2f24ad9c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 connection\n",
    "s3 = boto3.client(\n",
    "    service_name = \"s3\",\n",
    "    region_name = \"eu-west-3\",\n",
    "    aws_access_key_id = aws_access_key_id,\n",
    "    aws_secret_access_key = aws_secret_access_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107da57-87a8-4194-b1b6-def15a930de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific bucket data\n",
    "bucket_name = \"hotel-resa-prediction\"\n",
    "prefix = \"datasets/\"\n",
    "filename = \"processed_hotel_bookings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc4251-a172-4613-9eac-799c07391fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datas \n",
    "result = s3.list_objects(Bucket=bucket_name)\n",
    "for obj in result.get('Contents'):\n",
    "    if (obj[\"Key\"].startswith(prefix)) and (obj[\"Key\"].endswith(filename)):\n",
    "        data = s3.get_object(Bucket=bucket_name, Key=obj.get('Key'))\n",
    "        contents = data['Body'].read().decode(\"utf-8\")\n",
    "        data = pd.read_csv(io.StringIO(contents), low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f7f01",
   "metadata": {},
   "source": [
    "## Préparation des données pour l'ingestion dans la pipeline de machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac7827-1454-45a3-839a-95d0ab043b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"is_canceled\",  axis=1)\n",
    "y = data[\"is_canceled\"]\n",
    "print(\"Features : \", X.shape)\n",
    "print(\"Target : \", y.shape) # s'assurer de n'avoir qu'une colonne ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce37a6-eb9d-432d-a876-36b135579fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    shuffle=True, \n",
    "    stratify=y # IMPORTANT pour bien balancer les prédictions première itération --> courbe ROC mauvais \n",
    ")\n",
    "print(\"Features train : \", X_train.shape)\n",
    "print(\"Target train : \", y_train.shape)\n",
    "print(\"Features test : \", X_test.shape)\n",
    "print(\"Target test : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30489395-c81f-4807-a816-76c99dcbba8e",
   "metadata": {},
   "source": [
    "## Récupération du modèle dans le stockage objet AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb8df8-e5cb-418a-ab2f-fe8aafd2d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_model = \"models/\"\n",
    "filename_model = \"hotel_bookings_churn_model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3af3f-95aa-48c8-bdca-ad25e51e892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_model = s3.list_objects(Bucket=bucket_name)\n",
    "for obj in result_model.get('Contents'):\n",
    "    if (obj[\"Key\"].startswith(prefix_model)) and (obj[\"Key\"].endswith(filename_model)):\n",
    "        model = s3.get_object(Bucket=bucket_name, Key=obj.get('Key'))\n",
    "        model_content = model['Body'].read()\n",
    "        model_pipeline = joblib.load(io.BytesIO(model_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb9fd4-25cb-4b3c-b1ce-0bfdb6727a22",
   "metadata": {},
   "source": [
    "## Affinage du modèle\n",
    "> Précédemment, nous avons fait une comparaison par GridSearch et CrossValidation pour trouver le meilleur modèle de machine learning pour cette problématique de classification sans toucher aux hyperparamètres\n",
    "> Actuellement, nous allons nous occupé d'affiner les hyperparamètres du modèle pour avoir une meilleure performance. \n",
    "\n",
    "Pour se faire, nous allons utiliser le RandomizedSearchCV soit une validation croisée avec des échantillonnés aléatoirement dans un espace défini, permettant une exploration plus rapide qu'une recherche exhaustive. (un premier test trop lent a été réalisé avec GridSearchCV). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9625fc",
   "metadata": {},
   "source": [
    "**Affichage du modèle :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ad3ba-6329-4d58-9655-cf994febbb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7435c5e",
   "metadata": {},
   "source": [
    "**Info :**\n",
    "Récupération des hyperparamètre à affiner en fonction des éléments dans la [documentation scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) l'idée est de faire une sélection en prenant en compte la puissance de calcul de la machine à disposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47899e9-2442-48de-89e4-3457a562f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_distrib = {\n",
    "    \"gradientboostingclassifier__n_estimators\": [100, 200, 300],\n",
    "    \"gradientboostingclassifier__max_depth\": [3, 5, 7],\n",
    "    \"gradientboostingclassifier__learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"gradientboostingclassifier__subsample\": [0.8, 1.0],\n",
    "    \"gradientboostingclassifier__max_features\": [\"sqrt\", None],\n",
    "    \"gradientboostingclassifier__min_samples_split\": [5, 10],\n",
    "    \"gradientboostingclassifier__min_samples_leaf\": [1, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4a68c-4cea-43a9-9116-7e5cfb77f76b",
   "metadata": {},
   "source": [
    "**Info** : paramétrage de la validation croisée avec des échantillon aléatoire. Ici, on prend l'accuracy en méthode de scoring pour la sélection. L'idée est de faire 3 croisement et paraléliser les calculs avec `n_jobs=-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0eb3c8-9354-4424-ac9b-1f7d98b7f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cv = RandomizedSearchCV(\n",
    "    model_pipeline,\n",
    "    param_distributions=params_distrib,\n",
    "    n_iter=50,  \n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    error_score='raise'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b423327-1f23-420d-b54e-f5215fa1136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cv.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f102d47-9749-42dc-9934-0c88d2608bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = search_cv.cv_results_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1862acc-60c9-4b23-96de-a3f961aacfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_results).sort_values(by=\"rank_test_score\", ascending=True).head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc714b2",
   "metadata": {},
   "source": [
    "**Conclusions :** Quand on augmente le n_estimators, le score du modèle devient meilleur. Le modèle apprend donc mieux avec plus d’arbres. Une grand max_depth, comme 7, donne de bons résultats. Ça veut dire que le modèle a besoin de bien creuser pour repérer les bons motifs. Un learning_rate plus élevé (0.2) marche bien ici. Le modèle apprend plus vite et ça lui réussit. Un subsample à 0.8 (au lieu de 1.0) donne de meilleurs scores. Laisser un peu de hasard aide à éviter que le modèle s’adapte trop aux données. Les meilleurs scores sont avec min_samples_split et min_samples_leaf à 5. Donc, laisser le modèle créer plus facilement des divisions dans les arbres semble être un bon choix ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0081c-1e18-4ab3-bfe2-c65cf638ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = search_cv.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053babc-13fe-41bb-99b7-05acb610b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = best_model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d774f-a2c3-4ce0-95e2-1aebac832916",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, result)\n",
    "precision = precision_score(y_test, result)\n",
    "recall = recall_score(y_test, result)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36234148-d50e-4945-a11a-29c79d906a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy score : {accuracy*100:.3f}%\")\n",
    "print(f\"The precision score: {precision*100:.3f}%\")\n",
    "print(f\"The recall score: {recall*100:.3f}%\")\n",
    "print(f\"The ROC AUC score: {roc_auc*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9ce022",
   "metadata": {},
   "source": [
    "**Conclusion :**\n",
    "- Accuracy : montre que le modèle le modèle fait presque 88 bonnes prédictions sur 100.\n",
    "- Precision : Quand le modèle prédit une annulation, il a raison dans 86 % des cas. Il évite donc plutôt bien les fausses alertes.\n",
    "- Recall : montre que le modèle détecte environ 81 % des vraies annulations. Cela signifie qu’il en rate encore quelques-unes (faux négatifs).\n",
    "- ROC AUC : Le modèle est très bon pour distinguer les cas d’annulation des autres. Un score proche de 100 % montre une excellente capacité de séparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25810e24-4231-49af-a878-c988a4884dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(y_test, result) \n",
    "_ = sns.heatmap((confusion_matrix), annot=True).set(title=\"Matrice de confusion\", xlabel=\"Prédictions\", ylabel=\"Réel\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34fe45",
   "metadata": {},
   "source": [
    "**Conclusion :**\n",
    "Le modèle fait très peu d’erreurs dans l’ensemble. Il reconnaît bien les annulations (avec plus de 11 000 bonnes prédictions) mais en rate encore environ 2 600, ce qui correspond au recall d'environ 81 % vu plus tôt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2e4a3-2f10-4d94-a97d-7edd5aee6fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2)\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"Taux de faux positif\")\n",
    "plt.ylabel(\"Taux de vrai positif\")\n",
    "plt.title(\"Courbe ROC\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff92a48",
   "metadata": {},
   "source": [
    "**Conclusion :** la courbe ROC monte presque directement vers le coin supérieur gauche, nettement au-dessus de la diagonale hasard, ce qui montre que le modèle sépare très efficacement les réservations annulées des non-annulées.<br>\n",
    "**Décision :** nous conservons le seuil actuel 0,50 afin de limiter les fausses alertes, cela évite de déclencher à tort le futur filtre dans le formulaire de réservation tout en atteignant l’objectif métier principal : réduire les annulations réelles et donc les pertes de revenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"models/\"\n",
    "filename_model = \"finetuned_hotel_bookings_churn_model.pkl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ee84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(prefix, exist_ok=True)\n",
    "    joblib.dump(best_model, prefix+filename_model)\n",
    "    print(f\"Le modèle optimisé a été enregistré avec succès sous le nom {filename_model}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur dans l'enregistrement du modèle {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(prefix + filename_model, bucket_name, prefix + filename_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ee5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(prefix)\n",
    "os.remove(filename_model)\n",
    "print(\"models : \")\n",
    "print(\"- - - \" * 3)\n",
    "if len(os.listdir()) > 1:\n",
    "        for file in os.listdir():\n",
    "            print(file)\n",
    "else: \n",
    "    print(\"You'll retreive all models in AWS S3 !\")\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17436a90-ae1b-4a19-9034-77e8fc28c96c",
   "metadata": {},
   "source": [
    "-- END --"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
